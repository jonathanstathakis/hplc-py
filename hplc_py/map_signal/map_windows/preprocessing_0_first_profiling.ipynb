{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Profiling\n",
    "\n",
    "To develop a functional deconvolution pipeline, we need to optimize peak detection and windowing. All peaks need to be detected to fit each to the skew norm model, peak regions need to be seperated enough (either through sharpening or baseline subtraction) to deem them not convolved - seperated into different windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandera as pa\n",
    "import hvplot\n",
    "import hvplot.pandas  # noqa\n",
    "import panel as pn\n",
    "import polars as pl\n",
    "from dataclasses import dataclass\n",
    "from hplc_py.pipeline.preprocess_dashboard import DataSets, PreProcesser\n",
    "from pandera.typing.polars import LazyFrame, DataFrame\n",
    "\n",
    "pl.Config.set_tbl_rows(10)\n",
    "pn.extension()\n",
    "pn.config.comms = \"vscode\"\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = DataSets()\n",
    "ringland = dsets.ringland.fetch()\n",
    "ringland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = PreProcesser()\n",
    "prepro.ingest_signal(ringland, time_col=\"time\", amp_col=\"signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro.signal_adjustment(bcorr__n_iter=39)\n",
    "prepro.map_peaks(find_peaks_kwargs={\"prominence\": 0.01})\n",
    "prepro.map_windows()\n",
    "prepro.viz_preprocessing(opt_args={\"height\": 500, \"width\": 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, an insufficient number of peaks have been correctly detected, and many peaks have shoulders which are obviously highly convolved peaks. We need to increase the resolution through sharpening to maximise the peak detection and definition prior to windowing.\n",
    "\n",
    "The two factors for optimizing windowing are peak detection, peak width measurement, and basseline subtraction.\n",
    "\n",
    "Peak detection can be incresed by decreasing the minimum prominence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro.signal_adjustment(bcorr__n_iter=39)\n",
    "prepro.map_peaks(find_peaks_kwargs={\"prominence\": 0.0001})\n",
    "prepro.map_windows()\n",
    "prepro.viz_preprocessing(opt_args={\"height\": 1000, \"width\": 1500}).opts(\n",
    "    height=750, width=1250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Have doubled the number of peaks detected. But there is still a shoulder on peak zero that isnt detected, another small one on peak two, 44, 45, etc. I think a default sharpening is necessary. Notes continue in [Smoothing and Sharpening](../pipeline/preprocesing_1_smoothing_sharpening.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-hplc_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
